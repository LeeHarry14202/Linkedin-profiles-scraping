{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "#The sleep function helps slow down the execution from out script for given number of seconds\n",
    "import time\n",
    "from time import sleep"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Task 1: Login to Linkedin"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Load Chrome driver\n",
    "driver = webdriver.Chrome('/home/lee/Desktop/Linkedin-profiles-scraping-main/chromedriver')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "source": [
    "def go_to_linkedin():\n",
    "    # Link of web\n",
    "    url = 'https://www.linkedin.com/login'\n",
    "    #Open web   \n",
    "    driver.get(url)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "source": [
    "credential = open('login_credential.txt')\n",
    "#Read each line in login_credential.txt\n",
    "line = credential.readlines()\n",
    "username = line[0]\n",
    "password = line[1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "source": [
    "#Autofill email\n",
    "def autofill_email():\n",
    "    email_field = driver.find_element_by_xpath('//*[@id=\"username\"]')\n",
    "    email_field.send_keys(username)\n",
    "    sleep(2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "source": [
    "#Autofill password\n",
    "def autofill_password():\n",
    "    password_field_name = 'session_password'\n",
    "    password_field = driver.find_element_by_name(password_field_name)\n",
    "    password_field.send_keys(password)\n",
    "    sleep(3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "source": [
    "#Auto click sign in\n",
    "def autoclick_sign_in():\n",
    "    login_field_xpath = '//*[@id=\"organic-div\"]/form/div[3]/button'\n",
    "    login_field = driver.find_element_by_xpath(login_field_xpath)\n",
    "    login_field.click()\n",
    "    sleep(2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "source": [
    "def auto_login():\n",
    "    autofill_email()\n",
    "    autofill_password()\n",
    "    autoclick_sign_in()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#  Task2: Search for the profile we want to crawl"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "source": [
    "def see_all_people_results(): \n",
    "    class_=\"artdeco-pill\"\n",
    "    see_all_people_results = driver.find_element_by_class_name(class_)\n",
    "    see_all_people_results.click()\n",
    "    sleep(2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "source": [
    "def search(): \n",
    "    #The xpath of the search bar\n",
    "    search_field_xpath = '//*[@id=\"global-nav-typeahead\"]/input'\n",
    "    #Locate the search bar element\n",
    "    search_field = driver.find_element_by_xpath(search_field_xpath)\n",
    "    #Input search query to the search bar\n",
    "    search_query = 'software engineer people'#input('What profile do you want to search: ')\n",
    "    #Input text to search bar\n",
    "    search_field.send_keys(search_query)\n",
    "    sleep(2)\n",
    "    \n",
    "    #Search\n",
    "    search_field.send_keys(Keys.ENTER)\n",
    "    sleep(3)\n",
    "\n",
    "    see_all_people_results()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Task 3: Open the URLs of the profiles"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "source": [
    "#Pull data out of HTML files, which is the file that stores the content of website\n",
    "from bs4 import BeautifulSoup"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "source": [
    "def getURL():\n",
    "    # Load page_source\n",
    "    page_source = BeautifulSoup(driver.page_source)\n",
    "    # Locate URL profile\n",
    "    profiles = page_source.find_all('a',class_='app-aware-link')\n",
    "    # Create a list will contains URL profile on one page\n",
    "    list_profile_URL = []\n",
    "    max_profile_one_page = 10\n",
    "    profile_count_one_page = 0\n",
    "    remove_Linkedin_Member = 'https://www.linkedin.com/search/results/people/headless?origin=SWITCH_SEARCH_VERTICAL&keywords=software%20engineer%20people'\n",
    "    # Use loop to get many URL profile on one page\n",
    "    for profile in profiles:\n",
    "        # href address contains URL profile\n",
    "        profile_URL  =profile.get('href')   \n",
    "        if profile_URL not in list_profile_URL and profile_URL != remove_Linkedin_Member:\n",
    "            # Add URL profile to list\n",
    "            list_profile_URL.append(profile_URL)        \n",
    "            profile_count_one_page += 1\n",
    "        if profile_count_one_page == max_profile_one_page:\n",
    "            #if True stop the loop\n",
    "            break\n",
    "    #Remove href address, usually is the first element in list\n",
    "    list_profile_URL.pop(0)\n",
    "    return list_profile_URL\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "source": [
    "def getURLsonPages(number_of_page):\n",
    "    # Create a list will contains many URL profiles\n",
    "    URLs_all_page = []\n",
    "    for page in range(number_of_page):\n",
    "        URLs_one_page = getURL()\n",
    "        sleep(3)\n",
    "        \n",
    "        # Scroll to next button, if don't have this, it will not define where is the next button\n",
    "        scroll_to_next_button = 'window.scrollTo(0,document.body.scrollHeight);'\n",
    "        driver.execute_script(scroll_to_next_button)\n",
    "        sleep(3)\n",
    "\n",
    "        # Click the next button\n",
    "        next_button_class = 'artdeco-pagination__button--next'\n",
    "        next_button = driver.find_element_by_class_name(next_button_class)\n",
    "        next_button.click()\n",
    "        sleep(3)\n",
    "\n",
    "        # Add list one page to all page\n",
    "        URLs_all_page  = URLs_all_page + URLs_one_page\n",
    "        sleep(3)\n",
    "    return URLs_all_page"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Task 4: Scrape each profile & Write the data to a .CSV file"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 4.1: Write a function to access and scrape the data of 1 Linkedin profile"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "source": [
    "def get_personal_info(personal_linkedin_URL):\n",
    "    # for personal_linkedin_URL in URLs_all_page:\n",
    "    # Go to each URL page\n",
    "    driver.get(personal_linkedin_URL)\n",
    "    sleep(2)\n",
    "    # Get current URL page source\n",
    "    page_source = BeautifulSoup(driver.page_source,'html.parser')\n",
    "    # Get info tag \n",
    "    info_div = page_source.find('div',class_='pv-text-details__left-panel mr5')\n",
    "    # Get name tag from info tag by get_text() function and remove blank space with strip()\n",
    "    name = info_div.find('h1').get_text().strip()\n",
    "    # Same name tag\n",
    "    current_position = info_div.find(class_ = 'text-body-medium break-words').get_text().strip()\n",
    "    # Same name tag\n",
    "    country = info_div.find(class_ = 'text-body-small inline t-black--light break-words').get_text().strip()\n",
    "\n",
    "    # Create a list contains 3 values: name, current_position, country\n",
    "    list_personal_info = []\n",
    "    list_personal_info.append(name)\n",
    "    list_personal_info.append(current_position)\n",
    "    list_personal_info.append(country)\n",
    "    \n",
    "    # Return list personal info of one person \n",
    "    return list_personal_info\n",
    "\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 4.2: Write the output to a .CSV file"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "source": [
    "import csv"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "source": [
    "def export_output_to_csv():\n",
    "    number_of_page = 100 #int(input('How many pages you want to scrape: '))\n",
    "    URLs_all_page = getURLsonPages(number_of_page)\n",
    "\n",
    "    # Open and close  file output.csv \n",
    "    with open('output.csv', 'w', newline = '') as file_output:\n",
    "        #Create 4 headers tittle\n",
    "        headers = ['Name', 'Current Position', 'Country', 'URL']\n",
    "        #Write profile to file csv by DictWriter \n",
    "        writer = csv.DictWriter(file_output, delimiter=',', lineterminator ='\\n', fieldnames = headers)\n",
    "        #Write headers to file csv\n",
    "        writer.writeheader()\n",
    "\n",
    "        # Get info profile and write it to csv file in URL_all_page\n",
    "        for personal_linkedin_URL in URLs_all_page:\n",
    "            # personal_info is a list have 3 values: name, current_position, country\n",
    "            personal_info = get_personal_info(personal_linkedin_URL)\n",
    "            \n",
    "            #Situation index in personal_info list\n",
    "            name_index = 0 \n",
    "            current_position_index = 1\n",
    "            country_index = 2\n",
    "            name = personal_info[name_index]\n",
    "            current_position = personal_info[current_position_index]\n",
    "            country = personal_info[country_index]\n",
    "            \n",
    "            writer.writerow({headers[0]:name, headers[1]:current_position, headers[2]: country, headers[3]:personal_linkedin_URL})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "source": [
    "if __name__=='__main__':\n",
    "    go_to_linkedin()\n",
    "    auto_login()\n",
    "    search()\n",
    "    export_output_to_csv()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}